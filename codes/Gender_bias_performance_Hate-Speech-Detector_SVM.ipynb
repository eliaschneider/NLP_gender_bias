{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC #, SVC\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italian Datasets \n",
    "####################\n",
    "# HaSpeeDe: http://www.di.unito.it/~tutreeb/haspeede-evalita18/index.html\n",
    "TW_train = './datasets/HaSpeeDe/haspeede_TW-train.tsv'\n",
    "FB_train = './datasets/HaSpeeDe/haspeede_FB-train.tsv'\n",
    "TW_test = './datasets/HaSpeeDe/haspeede_TW-test.tsv'\n",
    "FB_test = './datasets/HaSpeeDe/haspeede_FB-test.tsv'\n",
    "# HSC: https://github.com/aitor-garcia-p/hate-speech-dataset\n",
    "HS_text = './datasets/HSC_with_text.csv'\n",
    "\n",
    "# English Datasets\n",
    "####################\n",
    "# StormfrontWS: https://github.com/aitor-garcia-p/hate-speech-dataset\n",
    "forum_post_eng = './datasets/StormfrontWS'\n",
    "# Davidson: https://github.com/t-davidson/hate-speech-and-offensive-language\n",
    "davidson_eng = './datasets/Davidson_hate_speech.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper here    https://www.rug.nl/research/portal/files/74389682/paper40.pdf\n",
    "# download here https://drive.google.com/drive/folders/133EPm4mO9dN6A0Cw6A6Sx1ABa-25BI8e\n",
    "embedding_ita1_file = './embeddings/model_hate_300.bin' \n",
    "# paper here    http://ceur-ws.org/Vol-2263/paper013.pdf\n",
    "# download here http://www.italianlp.it/resources/italian-word-embeddings/\n",
    "embedding_ita2_file = './embeddings/itwac128.sqlite' \n",
    "# paper here    http://ceur-ws.org/Vol-1404/paper_11.pdf\n",
    "# download here http://hlt.isti.cnr.it/wordembeddings/\n",
    "embedding_ita3_file = './embeddings/wiki_w2v/wiki_iter=5_algorithm=skipgram_window=10_size=300_neg-samples=10.m'\n",
    "# paper here    https://arxiv.org/pdf/1301.3781.pdf\n",
    "# download here https://code.google.com/archive/p/word2vec/\n",
    "embedding_eng1_file = './embeddings/GoogleNews-vectors-negative300.bin.gz'\n",
    "# paper here    https://nlp.stanford.edu/pubs/glove.pdf\n",
    "# download here https://nlp.stanford.edu/projects/glove/\n",
    "embedding_eng2_file = './embeddings/glove.6B.300d.txt'\n",
    "\n",
    "\n",
    "# Gender neutral built datasets: \n",
    "bias_samples_ita = './support_files/bias_ita_libs/dataset_bias_test.csv'\n",
    "bias_samples_eng = './support_files/bias_eng_libs/dataset_bias_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get files and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_haspeed_file(file, X, Y):\n",
    "    # Load train data from the FB file\n",
    "    samples, labels = [],[]\n",
    "\n",
    "    # Load train data from the TW file\n",
    "    with open(file,'r', encoding='utf-8') as fi:\n",
    "        for line in fi:\n",
    "            data = line.strip().split('\\t')\n",
    "            # get sample\n",
    "            samples.append(data[1])\n",
    "            # get labels\n",
    "            labels.append(data[2])\n",
    "    X += samples\n",
    "    Y += labels\n",
    "    return X, Y\n",
    "\n",
    "def get_StormfrontWS_files(directory, mode='train'):\n",
    "    all_files = []\n",
    "    if mode == 'train': \n",
    "        all_files = [os.path.join(directory, 'sampled_train', file) \n",
    "                     for file in os.listdir(os.path.join(directory, 'sampled_train'))]\n",
    "    if mode == 'test':\n",
    "        all_files = [os.path.join(directory, 'sampled_test', file) for file in \n",
    "                     os.listdir(os.path.join(directory, 'sampled_test'))]\n",
    "    if mode == 'all': \n",
    "        all_files = [os.path.join(directory, 'all_files', file)  \n",
    "                     for file in os.listdir(os.path.join(directory, 'all_files'))]\n",
    "    \n",
    "    samples = []\n",
    "    labels = []\n",
    "    annotations = pd.read_csv(os.path.join(directory, 'annotations_metadata.csv'))\n",
    "    for file in all_files:\n",
    "        with open(file,'r', encoding='utf-8') as fi:\n",
    "            samples.append(fi.readlines()[0])\n",
    "        file_id = file.split('/')[-1].split('.')[0]\n",
    "        if annotations[annotations['file_id'] == file_id]['label'].iloc[0] == 'noHate':\n",
    "            labels.append('0')\n",
    "        else:\n",
    "            labels.append('1')\n",
    "\n",
    "    return samples, labels \n",
    "\n",
    "def get_HSC_file(file): \n",
    "    samples, labels = [],[]\n",
    "    lines = pd.read_csv(file, encoding='utf-8')\n",
    "    for _, line in lines.iterrows():\n",
    "        # get sample\n",
    "        samples.append(line['text'])\n",
    "        # get labels\n",
    "        if line['hate speech'] == 'no': \n",
    "            labels.append('0')\n",
    "        else: \n",
    "            labels.append('1')\n",
    "    return samples, labels\n",
    "\n",
    "def get_davidson_file(file): \n",
    "    samples, labels = [],[]\n",
    "    lines = pd.read_csv(file, encoding='utf-8')\n",
    "    for _, line in lines.iterrows():\n",
    "        # get sample\n",
    "        samples.append(line['tweet'])\n",
    "        # get labels\n",
    "        if line['class'] == 2:\n",
    "            labels.append('0')\n",
    "        else: \n",
    "            labels.append('1')\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(embedding_file): \n",
    "    embeds = KeyedVectors.load(embedding_file).wv\n",
    "    vocab = {word for word in embeds.index2word}\n",
    "    return embeds, vocab \n",
    "\n",
    "\n",
    "def get_embedding2(embedding_file): \n",
    "    return Word2Vec.load(embedding_file).wv\n",
    "\n",
    "def load_wordvec_model(modelFile, flagBin):\n",
    "    model = KeyedVectors.load_word2vec_format(os.path.join(modelFile), binary=flagBin)\n",
    "    return model\n",
    "\n",
    "def get_embedding_sqlite(input_file):\n",
    "    embedding = dict()\n",
    "    #outfile = codecs.open(sys.argv[2], 'w', 'utf-8')\n",
    "    with  sqlite3.connect(input_file) as conn:\n",
    "        c = conn.cursor()\n",
    "        for row in c.execute(\"SELECT * from store\"):\n",
    "            if row[0] == \"\\t\":\n",
    "                continue\n",
    "            embedding[row[0]] = row[1:-1]\n",
    "    return embedding\n",
    "\n",
    "def load_glove_embeddings(filename):\n",
    "    with open(embedding_eng2_file, 'r') as f_embed:\n",
    "        vocab = [line.split()[0] for line in f_embed]\n",
    "    with open(embedding_eng2_file, 'r') as f_embed:\n",
    "        vec = [ [float(elem) for elem in line.split()[1:]] for line in f_embed]\n",
    "    embedding = {w: v for w, v in zip(vocab, vec)}\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_italian_text(samples):\n",
    "    new_samples = []\n",
    "    for string in samples:\n",
    "        string = re.sub(r'@\\S+','User', string)\n",
    "        string = re.sub(r'http\\S+', '', string)\n",
    "        string = re.sub(r'\\|LBR\\|', '', string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        string = re.sub(r\"'\", \" ' \", string)\n",
    "        string = string.replace(\"/\",\" \")\n",
    "        string = string.replace(\"-\",\" \")\n",
    "\n",
    "        string = string.replace(\"%\",\" percento \")\n",
    "        #string = re.sub(r\"[^A-Za-z0-9(),!?èéàòùì\\'\\`]\", \" \", string)\n",
    "        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('italian')) + r')\\b\\s*')\n",
    "        string = pattern.sub('', string)\n",
    "        string = string.strip().lower()\n",
    "        new_samples.append(string)\n",
    "    return new_samples\n",
    "\n",
    "def cleanup_english_text(samples):\n",
    "    new_samples = []\n",
    "    for string in samples:\n",
    "        string = re.sub(r'@\\S+','User', string)\n",
    "        string = re.sub(r'http\\S+', '', string)\n",
    "        string = re.sub(r'\\|LBR\\|', '', string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        string = string.replace(\"/\",\" \")\n",
    "\n",
    "        string = string.replace(\"'s\", \" \")\n",
    "        string = string.replace(\"n't\", \" not \")\n",
    "        string = string.replace(\"'ve\", \" have \")\n",
    "        string = string.replace(\"'re\", \" are \")\n",
    "        string = string.replace(\"I'm\",\" I am \")\n",
    "        string = string.replace(\"you're\",\" you are \")\n",
    "        string = string.replace(\"You're\",\" You are \")\n",
    "\n",
    "        string = string.replace(\"%\",\" percent \")\n",
    "        #string = re.sub(r'[^a-zA-Z0-9 ]', '', string)\n",
    "        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "        string = pattern.sub('', string)\n",
    "        string = string.strip().lower()\n",
    "        new_samples.append(string)\n",
    "    return new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(Ytest, Yguess): \n",
    "    scores = metrics.precision_recall_fscore_support(Ytest, Yguess, average='binary', pos_label='1')\n",
    "    print('precision: {}'.format(scores[0]))\n",
    "    print('Recall:    {}'.format(scores[1]))\n",
    "    print('F1-score:  {}'.format(scores[2]))\n",
    "\n",
    "def get_confusion_metrics(Ytest, Yguess):\n",
    "    conf_mat = metrics.confusion_matrix(Ytest, Yguess)\n",
    "    print('\\n Confusion Matrix')\n",
    "    print(' pred \\ true | nohate | hate ')\n",
    "    print('      -----------------------')\n",
    "    print('      nohate |   {}  | {} '.format(conf_mat[0][0],conf_mat[0][1]))\n",
    "    print('        hate |   {}  | {} '.format(conf_mat[1][0],conf_mat[1][1]))\n",
    "    \n",
    "def get_gender_bias_metrics(bias_test_df): \n",
    "    print('Metrics related to full dataset')\n",
    "    tot_pos = bias_test_df[bias_test_df['correct'] == 0]\n",
    "    Yfull = bias_test_df['correct'].tolist() \n",
    "    Ypred = bias_test_df['pred'].tolist() \n",
    "    get_metrics(Yfull, Ypred)\n",
    "    tp, fp, fn, tn = metrics.confusion_matrix(Yfull, Ypred).ravel()\n",
    "    fpr = fp / (tp + fp)\n",
    "    fnr = fn / (tn + fn)\n",
    "    \n",
    "    print('\\n\\nMetrics related to male dataset')\n",
    "    Ymale = bias_test_df[bias_test_df['gender'] == 'male']['correct'].tolist() \n",
    "    Ypred = bias_test_df[bias_test_df['gender'] == 'male']['pred'].tolist() \n",
    "    get_metrics(Ymale, Ypred)\n",
    "    tp_m, fp_m, fn_m, tn_m = metrics.confusion_matrix(Ymale, Ypred).ravel()\n",
    "    fpr_m = fp_m / (tp_m + fp_m)\n",
    "    fnr_m = fn_m / (tn_m + fn_m)\n",
    "    \n",
    "    print('\\n\\nMetrics related to female dataset')\n",
    "    Yfemale = bias_test_df[bias_test_df['gender'] == 'female']['correct'].tolist() \n",
    "    Ypred = bias_test_df[bias_test_df['gender'] == 'female']['pred'].tolist()\n",
    "    get_metrics(Yfemale, Ypred)\n",
    "    tp_f, fp_f, fn_f, tn_f = metrics.confusion_matrix(Yfemale, Ypred).ravel()\n",
    "    fpr_f = fp_f / (tp_f + fp_f)\n",
    "    fnr_f = fn_f / (tn_f + fn_f)\n",
    "    \n",
    "    print('\\n\\nGender bias metrics')\n",
    "    print(' False Positive Equality Difference (FPED): {}'.format(abs(fpr - fpr_m) + abs(fpr - fpr_f)))\n",
    "    print(' False Negative Equality Difference (FNED): {}'.format(abs(fnr - fnr_m) + abs(fnr - fnr_f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to apply embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(TransformerMixin):\n",
    "    '''Transformer object turning a sentence into a single embedding vector'''\n",
    "\n",
    "    def __init__(self, word_embeds, pool='max'):\n",
    "        '''\n",
    "        Required input: word embeddings stored in dict structure available for look-up\n",
    "        pool: sentence embeddings to be obtained either via average pooling ('average') or max pooing ('max') from word embeddings. Default is average pooling.\n",
    "        '''\n",
    "        self.word_embeds = word_embeds\n",
    "        self.pool_method = pool\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        '''\n",
    "        Transformation function: X is list of sentence/tweet - strings in the train data. Returns list of embeddings, each embedding representing one tweet\n",
    "        '''\n",
    "        return [self.get_sent_embedding(sent, self.word_embeds, self.pool_method) for sent in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_sent_embedding(self, sentence, word_embeds, pool):\n",
    "        '''\n",
    "        Obtains sentence embedding representing a whole sentence / tweet\n",
    "        '''\n",
    "        # simply get dim of embeddings\n",
    "        l_vector = len(word_embeds['sport'])\n",
    "\n",
    "        # replace each word in sentence with its embedding representation via look up in the embedding dict strcuture\n",
    "        # if no word_embedding available for a word, just ignore the word\n",
    "        # [[0.234234,-0.276583...][0.2343, -0.7356354, 0.123 ...][0.2344356, 0.12477...]...]\n",
    "        list_of_embeddings = [word_embeds[word.lower()] for word in sentence.split() if word.lower() in word_embeds]\n",
    "\n",
    "\t    # Obtain sentence embeddings either by average or max pooling on word embeddings of the sentence\n",
    "        # Option via argument 'pool'\n",
    "        if pool == 'pool':\n",
    "            sent_embedding = [sum(col) / float(len(col)) for col in zip(*list_of_embeddings)]  # average pooling\n",
    "        elif pool == 'max':\n",
    "            sent_embedding = [max(col) for col in zip(*list_of_embeddings)]\t# max pooling\n",
    "        else:\n",
    "            raise ValueError('Unknown pooling method!')\n",
    "\n",
    "        # Handle small sentences not in word embedding\n",
    "        if len(sent_embedding) != l_vector:\n",
    "            sent_embedding = [0] * l_vector\n",
    "\n",
    "        return sent_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speed dataset - Italian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading EVALITA FB and TW training data\n"
     ]
    }
   ],
   "source": [
    "print('Reading EVALITA FB and TW training data')\n",
    "Xtrain = [] \n",
    "Ytrain = []\n",
    "Xtrain,Ytrain = get_haspeed_file(FB_train, Xtrain, Ytrain)\n",
    "Xtrain,Ytrain = get_haspeed_file(TW_train, Xtrain, Ytrain)\n",
    "Xtrain = cleanup_italian_text(Xtrain)\n",
    "\n",
    "Xtest = [] \n",
    "Ytest = []\n",
    "Xtest,Ytest = get_haspeed_file(FB_test, Xtest, Ytest)\n",
    "Xtest,Ytest = get_haspeed_file(TW_test, Xtest, Ytest)\n",
    "Xtest = cleanup_italian_text(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use embedding: HateSpeech Evalita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained word embeddings from [file name article]\n",
      "Preparing vectorizer and classifier\n",
      "Fitting on training data\n",
      "Predicting classifier of train data\n",
      "F1-score: 0.9991500212494687\n",
      "\n",
      "Predicting classifier of test data\n",
      "precision: 0.821656050955414\n",
      "Recall:    0.7732267732267732\n",
      "F1-score:  0.7967061245496655\n"
     ]
    }
   ],
   "source": [
    "print('Loading pretrained word embeddings from Merenda 2018')\n",
    "embeddings_ita1, vocab = get_embedding(embedding_ita1_file)\n",
    "\n",
    "print('Preparing vectorizer and classifier')\n",
    "count_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), binary=False, sublinear_tf=False)\n",
    "count_char = TfidfVectorizer(analyzer='char', ngram_range=(2,4), binary=False, sublinear_tf=False)\n",
    "\n",
    "vectorizer = FeatureUnion([('word', count_word), ('char', count_char), \n",
    "                           ('word_embeds', Embeddings(embeddings_ita1, pool='pool'))])\n",
    "\n",
    "clf = LinearSVC()\n",
    "classifier_ita1 = Pipeline([('vectorize', vectorizer), ('classify', clf)])\n",
    "\n",
    "print('Fitting on training data')\n",
    "classifier_ita1.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Predicting classifier of train data')\n",
    "Yguess = classifier_ita1.predict(Xtrain)\n",
    "scores = metrics.precision_recall_fscore_support(Ytrain, Yguess, average='binary', pos_label='1')\n",
    "print('F1-score: {}'.format(scores[2]))\n",
    "\n",
    "print('\\nPredicting classifier of test data')\n",
    "Yguess = classifier_ita1.predict(Xtest)\n",
    "get_metrics(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics related to full dataset\n",
      "precision: 0.930379746835443\n",
      "Recall:    0.441\n",
      "F1-score:  0.5983717774762551\n",
      "\n",
      "\n",
      "Metrics related to male dataset\n",
      "precision: 0.9868421052631579\n",
      "Recall:    0.45\n",
      "F1-score:  0.618131868131868\n",
      "\n",
      "\n",
      "Metrics related to female dataset\n",
      "precision: 0.8780487804878049\n",
      "Recall:    0.432\n",
      "F1-score:  0.5790884718498659\n",
      "\n",
      "\n",
      "Gender bias metrics\n",
      " False Positive Equality Difference (FPED): 0.054\n",
      " False Negative Equality Difference (FNED): 0.017999999999999905\n"
     ]
    }
   ],
   "source": [
    "bias_test_df = pd.read_csv(bias_samples_ita)\n",
    "bias_test_df['correct'] = ['0' if label == 'NOT_BAD' else '1' for label in bias_test_df['hate']]\n",
    "X = cleanup_italian_text(bias_test_df['text'])\n",
    "bias_test_df['pred'] = classifier_ita1.predict(X)\n",
    "\n",
    "get_gender_bias_metrics(bias_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Embeddings: Generic Evalita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained word embeddings from [file name article]\n",
      "Preparing vectorizer and classifier\n",
      "Fitting on training data\n",
      "Predicting classifier of train data\n",
      "F1-score: 0.9991500212494687\n",
      "\n",
      "Predicting classifier of test data\n",
      "precision: 0.8118393234672304\n",
      "Recall:    0.7672327672327672\n",
      "F1-score:  0.7889060092449922\n"
     ]
    }
   ],
   "source": [
    "print('Loading pretrained word embeddings from [file name article]')\n",
    "embeddings_ita2 = get_embedding_sqlite(embedding_ita2_file)\n",
    "\n",
    "print('Preparing vectorizer and classifier')\n",
    "count_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), binary=False, sublinear_tf=False)\n",
    "count_char = TfidfVectorizer(analyzer='char', ngram_range=(2,4), binary=False, sublinear_tf=False)\n",
    "\n",
    "vectorizer = FeatureUnion([('word', count_word), ('char', count_char), \n",
    "                           ('word_embeds', Embeddings(embeddings_ita2, pool='pool'))])\n",
    "\n",
    "clf = LinearSVC()\n",
    "classifier_ita2 = Pipeline([('vectorize', vectorizer), ('classify', clf)])\n",
    "\n",
    "print('Fitting on training data')\n",
    "classifier_ita2.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Predicting classifier of train data')\n",
    "Yguess = classifier_ita2.predict(Xtrain)\n",
    "scores = metrics.precision_recall_fscore_support(Ytrain, Yguess, average='binary', pos_label='1')\n",
    "print('F1-score: {}'.format(scores[2]))\n",
    "\n",
    "print('\\nPredicting classifier of test data')\n",
    "Yguess = classifier_ita2.predict(Xtest)\n",
    "get_metrics(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics related to full dataset\n",
      "precision: 0.9842696629213483\n",
      "Recall:    0.438\n",
      "F1-score:  0.6062283737024221\n",
      "\n",
      "\n",
      "Metrics related to male dataset\n",
      "precision: 0.9914893617021276\n",
      "Recall:    0.466\n",
      "F1-score:  0.6340136054421769\n",
      "\n",
      "\n",
      "Metrics related to female dataset\n",
      "precision: 0.9761904761904762\n",
      "Recall:    0.41\n",
      "F1-score:  0.5774647887323944\n",
      "\n",
      "\n",
      "Gender bias metrics\n",
      " False Positive Equality Difference (FPED): 0.006\n",
      " False Negative Equality Difference (FNED): 0.05599999999999994\n"
     ]
    }
   ],
   "source": [
    "bias_test_df = pd.read_csv(bias_samples_ita)\n",
    "bias_test_df['correct'] = ['0' if label == 'NOT_BAD' else '1' for label in bias_test_df['hate']]\n",
    "X = cleanup_italian_text(bias_test_df['text'])\n",
    "bias_test_df['pred'] = classifier_ita2.predict(X)\n",
    "\n",
    "get_gender_bias_metrics(bias_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use embedding: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained word embeddings from Berardi 2015\n",
      "Preparing vectorizer and classifier\n",
      "Fitting on training data\n",
      "Predicting classifier of train data\n",
      "F1-score: 0.9991496598639457\n",
      "\n",
      "Predicting classifier of test data\n",
      "precision: 0.8167202572347267\n",
      "Recall:    0.7612387612387612\n",
      "F1-score:  0.7880041365046536\n"
     ]
    }
   ],
   "source": [
    "print('Loading pretrained word embeddings from Berardi 2015')\n",
    "embeddings_ita3 = get_embedding2(embedding_ita3_file)\n",
    "\n",
    "print('Preparing vectorizer and classifier')\n",
    "count_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), binary=False, sublinear_tf=False)\n",
    "count_char = TfidfVectorizer(analyzer='char', ngram_range=(2,4), binary=False, sublinear_tf=False)\n",
    "\n",
    "vectorizer = FeatureUnion([('word', count_word), ('char', count_char), \n",
    "                           ('word_embeds', Embeddings(embeddings_ita3, pool='pool'))])\n",
    "\n",
    "clf = LinearSVC()\n",
    "classifier_ita3 = Pipeline([('vectorize', vectorizer), ('classify', clf)])\n",
    "\n",
    "print('Fitting on training data')\n",
    "classifier_ita3.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Predicting classifier of train data')\n",
    "Yguess = classifier_ita3.predict(Xtrain)\n",
    "scores = metrics.precision_recall_fscore_support(Ytrain, Yguess, average='binary', pos_label='1')\n",
    "print('F1-score: {}'.format(scores[2]))\n",
    "\n",
    "print('\\nPredicting classifier of test data')\n",
    "Yguess = classifier_ita3.predict(Xtest)\n",
    "get_metrics(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics related to full dataset\n",
      "precision: 0.8597972972972973\n",
      "Recall:    0.509\n",
      "F1-score:  0.6394472361809045\n",
      "\n",
      "\n",
      "Metrics related to male dataset\n",
      "precision: 0.9217081850533808\n",
      "Recall:    0.518\n",
      "F1-score:  0.6632522407170295\n",
      "\n",
      "\n",
      "Metrics related to female dataset\n",
      "precision: 0.8038585209003215\n",
      "Recall:    0.5\n",
      "F1-score:  0.6165228113440198\n",
      "\n",
      "\n",
      "Gender bias metrics\n",
      " False Positive Equality Difference (FPED): 0.078\n",
      " False Negative Equality Difference (FNED): 0.018000000000000016\n"
     ]
    }
   ],
   "source": [
    "bias_test_df = pd.read_csv(bias_samples_ita)\n",
    "bias_test_df['correct'] = ['0' if label == 'NOT_BAD' else '1' for label in bias_test_df['hate']]\n",
    "X = cleanup_italian_text(bias_test_df['text'])\n",
    "bias_test_df['pred'] = classifier_ita3.predict(X)\n",
    "\n",
    "get_gender_bias_metrics(bias_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StormfrontWS dataset - English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading StormfrontWS Web forum data\n"
     ]
    }
   ],
   "source": [
    "print('Reading StormfrontWS Web forum data')\n",
    "Xtrain, Ytrain = get_StormfrontWS_files(forum_post_eng, 'train')\n",
    "Xtrain = cleanup_english_text(Xtrain)\n",
    "\n",
    "Xtest, Ytest = get_StormfrontWS_files(forum_post_eng, 'test')\n",
    "Xtest = cleanup_english_text(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding word2vec Google News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_AP = load_wordvec_model(embedding_eng1_file, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing vectorizer and classifier\n",
      "Fitting on training data\n",
      "Predicting classifier of train data\n",
      "F1-score: 1.0\n",
      "\n",
      "Predicting classifier of test data\n",
      "precision: 0.7451737451737451\n",
      "Recall:    0.8075313807531381\n",
      "F1-score:  0.7751004016064258\n"
     ]
    }
   ],
   "source": [
    "print('Preparing vectorizer and classifier')\n",
    "count_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), binary=False, sublinear_tf=False)\n",
    "count_char = TfidfVectorizer(analyzer='char', ngram_range=(2,4), binary=False, sublinear_tf=False)\n",
    "\n",
    "vectorizer = FeatureUnion([('word', count_word), ('char', count_char), \n",
    "                           ('word_embeds', Embeddings(model_w2v_AP, pool='pool'))])\n",
    "\n",
    "clf = LinearSVC()\n",
    "classifier_eng1 = Pipeline([('vectorize', vectorizer), ('classify', clf)])\n",
    "\n",
    "print('Fitting on training data')\n",
    "classifier_eng1.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Predicting classifier of train data')\n",
    "Yguess = classifier_eng1.predict(Xtrain)\n",
    "scores = metrics.precision_recall_fscore_support(Ytrain, Yguess, average='binary', pos_label='1')\n",
    "print('F1-score: {}'.format(scores[2]))\n",
    "\n",
    "print('\\nPredicting classifier of test data')\n",
    "Yguess = classifier_eng1.predict(Xtest)\n",
    "get_metrics(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics related to full dataset\n",
      "precision: 0.8819277108433735\n",
      "Recall:    0.366\n",
      "F1-score:  0.5173144876325089\n",
      "\n",
      "\n",
      "Metrics related to male dataset\n",
      "precision: 0.9281437125748503\n",
      "Recall:    0.31\n",
      "F1-score:  0.46476761619190404\n",
      "\n",
      "\n",
      "Metrics related to female dataset\n",
      "precision: 0.8508064516129032\n",
      "Recall:    0.422\n",
      "F1-score:  0.5641711229946524\n",
      "\n",
      "\n",
      "Gender bias metrics\n",
      " False Positive Equality Difference (FPED): 0.049999999999999996\n",
      " False Negative Equality Difference (FNED): 0.11199999999999999\n"
     ]
    }
   ],
   "source": [
    "bias_test_df = pd.read_csv(bias_samples_eng)\n",
    "bias_test_df['correct'] = ['0' if label == 'NOT_BAD' else '1' for label in bias_test_df['hate']]\n",
    "X = cleanup_italian_text(bias_test_df['text'])\n",
    "bias_test_df['pred'] = classifier_eng1.predict(X)\n",
    "\n",
    "get_gender_bias_metrics(bias_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6B = load_glove_embeddings(embedding_eng2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing vectorizer and classifier\n",
      "Fitting on training data\n",
      "Predicting classifier of train data\n",
      "F1-score: 1.0\n",
      "\n",
      "Predicting classifier of test data\n",
      "precision: 0.7244094488188977\n",
      "Recall:    0.7698744769874477\n",
      "F1-score:  0.7464503042596349\n"
     ]
    }
   ],
   "source": [
    "print('Preparing vectorizer and classifier')\n",
    "count_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), binary=False, sublinear_tf=False)\n",
    "count_char = TfidfVectorizer(analyzer='char', ngram_range=(2,4), binary=False, sublinear_tf=False)\n",
    "\n",
    "vectorizer = FeatureUnion([('word', count_word), ('char', count_char), \n",
    "                           ('word_embeds', Embeddings(glove_6B, pool='pool'))])\n",
    "\n",
    "clf = LinearSVC()\n",
    "classifier_eng2 = Pipeline([('vectorize', vectorizer), ('classify', clf)])\n",
    "\n",
    "print('Fitting on training data')\n",
    "classifier_eng2.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Predicting classifier of train data')\n",
    "Yguess = classifier_eng2.predict(Xtrain)\n",
    "scores = metrics.precision_recall_fscore_support(Ytrain, Yguess, average='binary', pos_label='1')\n",
    "print('F1-score: {}'.format(scores[2]))\n",
    "\n",
    "print('\\nPredicting classifier of test data')\n",
    "Yguess = classifier_eng2.predict(Xtest)\n",
    "get_metrics(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics related to full dataset\n",
      "precision: 0.7566462167689162\n",
      "Recall:    0.37\n",
      "F1-score:  0.49697783747481533\n",
      "\n",
      "\n",
      "Metrics related to male dataset\n",
      "precision: 0.7958115183246073\n",
      "Recall:    0.304\n",
      "F1-score:  0.43994211287988416\n",
      "\n",
      "\n",
      "Metrics related to female dataset\n",
      "precision: 0.7315436241610739\n",
      "Recall:    0.436\n",
      "F1-score:  0.5463659147869674\n",
      "\n",
      "\n",
      "Gender bias metrics\n",
      " False Positive Equality Difference (FPED): 0.082\n",
      " False Negative Equality Difference (FNED): 0.132\n"
     ]
    }
   ],
   "source": [
    "bias_test_df = pd.read_csv(bias_samples_eng)\n",
    "bias_test_df['correct'] = ['0' if label == 'NOT_BAD' else '1' for label in bias_test_df['hate']]\n",
    "X = cleanup_italian_text(bias_test_df['text'])\n",
    "bias_test_df['pred'] = classifier_eng2.predict(X)\n",
    "\n",
    "get_gender_bias_metrics(bias_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# Tweet Hate Speech - Italian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set ration hate not hate 0.14929577464788732 over 1420\n",
      "Test set ration hate not hate 0.13 over 100\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_HSC_file(HS_text)\n",
    "\n",
    "test_i = set()\n",
    "while len(test_i) < 100: \n",
    "    test_i.add(int(np.random.uniform(len(X))))\n",
    "\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "Xtest = []\n",
    "Ytest = []\n",
    "for i in range(len(X)): \n",
    "    if i in list(test_i): \n",
    "        Xtest.append(X[i])\n",
    "        Ytest.append(Y[i])\n",
    "    else: \n",
    "        Xtrain.append(X[i])\n",
    "        Ytrain.append(Y[i])\n",
    "print('Train set ration hate not hate {} over {}'.format(sum([int(y) for y in Ytrain])/len(Ytrain), len(Xtrain)))\n",
    "print('Test set ration hate not hate {} over {}'.format(sum([int(y) for y in Ytest])/len(Ytest), len(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing vectorizer and classifier\n",
      "Fitting on training data\n",
      "Predicting classifier of train data\n",
      "F1-score: 1.0\n",
      "\n",
      "Predicting classifier of test data\n",
      "precision: 0.5\n",
      "Recall:    0.15384615384615385\n",
      "F1-score:  0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "embeddings_ita2 = get_embedding_sqlite(embedding_ita2_file)\n",
    "\n",
    "print('Preparing vectorizer and classifier')\n",
    "count_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), binary=False, sublinear_tf=False)\n",
    "count_char = TfidfVectorizer(analyzer='char', ngram_range=(2,4), binary=False, sublinear_tf=False)\n",
    "\n",
    "vectorizer = FeatureUnion([('word', count_word), ('char', count_char), \n",
    "                           ('word_embeds', Embeddings(embeddings_ita2, pool='pool'))])\n",
    "\n",
    "clf = LinearSVC()\n",
    "classifier_ita2 = Pipeline([('vectorize', vectorizer), ('classify', clf)])\n",
    "\n",
    "print('Fitting on training data')\n",
    "classifier_ita2.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Predicting classifier of train data')\n",
    "Yguess = classifier_ita2.predict(Xtrain)\n",
    "scores = metrics.precision_recall_fscore_support(Ytrain, Yguess, average='binary', pos_label='1')\n",
    "print('F1-score: {}'.format(scores[2]))\n",
    "\n",
    "print('\\nPredicting classifier of test data')\n",
    "Yguess = classifier_ita2.predict(Xtest)\n",
    "get_metrics(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics related to full dataset\n",
      "precision: 1.0\n",
      "Recall:    0.006\n",
      "F1-score:  0.011928429423459244\n",
      "\n",
      "\n",
      "Metrics related to male dataset\n",
      "precision: 1.0\n",
      "Recall:    0.004\n",
      "F1-score:  0.00796812749003984\n",
      "\n",
      "\n",
      "Metrics related to female dataset\n",
      "precision: 1.0\n",
      "Recall:    0.008\n",
      "F1-score:  0.015873015873015872\n",
      "\n",
      "\n",
      "Gender bias metrics\n",
      " False Positive Equality Difference (FPED): 0.0\n",
      " False Negative Equality Difference (FNED): 0.0040000000000000036\n"
     ]
    }
   ],
   "source": [
    "bias_test_df = pd.read_csv(bias_samples_ita)\n",
    "bias_test_df['correct'] = ['0' if label == 'NOT_BAD' else '1' for label in bias_test_df['hate']]\n",
    "X = cleanup_italian_text(bias_test_df['text'])\n",
    "bias_test_df['pred'] = classifier_ita2.predict(X)\n",
    "\n",
    "get_gender_bias_metrics(bias_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# Tweet Hate Speech - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set ration hate not hate 0.832484047192765 over 21783\n",
      "Test set ration hate not hate 0.8286666666666667 over 3000\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_davidson_file(davidson_eng)\n",
    "\n",
    "test_i = set()\n",
    "while len(test_i) < 3000: \n",
    "    test_i.add(int(np.random.uniform(len(X))))\n",
    "\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "Xtest = []\n",
    "Ytest = []\n",
    "for i in range(len(X)): \n",
    "    if i in list(test_i): \n",
    "        Xtest.append(X[i])\n",
    "        Ytest.append(Y[i])\n",
    "    else: \n",
    "        Xtrain.append(X[i])\n",
    "        Ytrain.append(Y[i])\n",
    "print('Train set ration hate not hate {} over {}'.format(sum([int(y) for y in Ytrain])/len(Ytrain), len(Xtrain)))\n",
    "print('Test set ration hate not hate {} over {}'.format(sum([int(y) for y in Ytest])/len(Ytest), len(Xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing vectorizer and classifier\n",
      "Fitting on training data\n",
      "Predicting classifier of train data\n",
      "F1-score: 1.0\n",
      "\n",
      "Predicting classifier of test data\n",
      "precision: 0.9719213798636182\n",
      "Recall:    0.9746580852775543\n",
      "F1-score:  0.9732878087969471\n"
     ]
    }
   ],
   "source": [
    "model_w2v_AP = load_wordvec_model(embedding_eng1_file, True)\n",
    "\n",
    "print('Preparing vectorizer and classifier')\n",
    "count_word = TfidfVectorizer(analyzer='word', ngram_range=(1,3), binary=False, sublinear_tf=False)\n",
    "count_char = TfidfVectorizer(analyzer='char', ngram_range=(2,4), binary=False, sublinear_tf=False)\n",
    "\n",
    "vectorizer = FeatureUnion([('word', count_word), ('char', count_char), \n",
    "                           ('word_embeds', Embeddings(model_w2v_AP, pool='pool'))])\n",
    "\n",
    "clf = LinearSVC()\n",
    "classifier_ita2 = Pipeline([('vectorize', vectorizer), ('classify', clf)])\n",
    "\n",
    "print('Fitting on training data')\n",
    "classifier_ita2.fit(Xtrain, Ytrain)\n",
    "\n",
    "print('Predicting classifier of train data')\n",
    "Yguess = classifier_ita2.predict(Xtrain)\n",
    "scores = metrics.precision_recall_fscore_support(Ytrain, Yguess, average='binary', pos_label='1')\n",
    "print('F1-score: {}'.format(scores[2]))\n",
    "\n",
    "print('\\nPredicting classifier of test data')\n",
    "Yguess = classifier_ita2.predict(Xtest)\n",
    "get_metrics(Ytest, Yguess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics related to full dataset\n",
      "precision: 0.46411483253588515\n",
      "Recall:    0.194\n",
      "F1-score:  0.2736248236953455\n",
      "\n",
      "\n",
      "Metrics related to male dataset\n",
      "precision: 0.45136186770428016\n",
      "Recall:    0.232\n",
      "F1-score:  0.30647291941875826\n",
      "\n",
      "\n",
      "Metrics related to female dataset\n",
      "precision: 0.484472049689441\n",
      "Recall:    0.156\n",
      "F1-score:  0.2360060514372163\n",
      "\n",
      "\n",
      "Gender bias metrics\n",
      " False Positive Equality Difference (FPED): 0.11599999999999996\n",
      " False Negative Equality Difference (FNED): 0.07599999999999996\n"
     ]
    }
   ],
   "source": [
    "bias_test_df = pd.read_csv(bias_samples_ita)\n",
    "bias_test_df['correct'] = ['0' if label == 'NOT_BAD' else '1' for label in bias_test_df['hate']]\n",
    "X = cleanup_italian_text(bias_test_df['text'])\n",
    "bias_test_df['pred'] = classifier_ita2.predict(X)\n",
    "\n",
    "get_gender_bias_metrics(bias_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
